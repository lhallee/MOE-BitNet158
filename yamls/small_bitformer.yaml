general_config:
  num_labels: !!int 10
  patience: !!int 3

model_config:
  vocab_size: !!int 32000
  hidden_size: !!int 512
  intermediate_size: !!int 2048
  num_hidden_layers: !!int 12
  num_attention_heads: !!int 8
  num_key_value_heads: !!int 8
  hidden_act: !!str silu
  max_position_embeddings: !!int 4096
  initializer_range: !!float 0.02
  rms_norm_eps: !!float 1e-5
  use_cache: false
  pad_token_id: null
  bos_token_id: 1
  eos_token_id: 2
  tie_word_embeddings: false
  rope_theta: !!float 1e6
  sliding_window: !!int 4096
  attention_dropout: !!float 0.0
  num_experts_per_tok: !!int 2
  num_local_experts: !!int 4
  output_router_logits: true
  router_aux_loss_coef: !!float 0.001
  attention_type: !!str sdpa
  is_causal: false
  moe: true
  bitnet: true

training_args:
  # typical args
  output_dir: !!str ./results
  logging_dir: !!str ./logs
  report_to: null
  evaluation_strategy: !!str epoch
  per_device_train_batch_size: !!int 2
  per_device_eval_batch_size: !!int 2
  gradient_accumulation_steps: !!int 1
  learning_rate: !!float 0.005
  weight_decay: !!float 0.01
  num_train_epochs: !!int 1
  max_steps: !!int 100
  warmup_ratio: !!float 0.0
  warmup_steps: !!int 100
  lr_scheduler_type: !!str cosine
  save_strategy: !!str epoch
  save_steps: !!int 1000
  save_total_limit: !!int 3
  dataloader_drop_last: false
  eval_steps: null
  logging_strategy: epoch
  logging_first_step: false
  logging_steps: !!int 100
  bf16: false
  fp16: false
  seed: !!int 42
  do_train: true
  do_eval: false
  eval_accumulation_steps: null
  group_by_length: false
  length_column_name: !!str length
  save_safetensors: true
