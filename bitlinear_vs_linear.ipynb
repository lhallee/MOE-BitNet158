{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import bitsandbytes as bnb\n",
    "import itertools\n",
    "from tabulate import tabulate\n",
    "from bitsandbytes.nn import Linear4bit, Params4bit\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNBLinear(Linear4bit):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_features,\n",
    "            output_features,\n",
    "            bias=False,\n",
    "            compute_dtype=None,\n",
    "            compress_statistics=True,\n",
    "            quant_type='fp4',\n",
    "            quant_storage=torch.uint8,\n",
    "            num_bits=8,\n",
    "            device=None):\n",
    "        super().__init__(input_features, output_features, bias, device)\n",
    "        self.weight = Params4bit(self.weight.data,\n",
    "                                 requires_grad=False,\n",
    "                                 compress_statistics=compress_statistics,\n",
    "                                 quant_type=quant_type,\n",
    "                                 quant_storage=quant_storage,\n",
    "                                 module=self)\n",
    "        self.norm: nn.Module = nn.LayerNorm(input_features)\n",
    "        self.compute_dtype = compute_dtype\n",
    "        self.compute_type_is_set = False\n",
    "        self.quant_state = None\n",
    "        self.quant_storage = quant_storage\n",
    "        self.eps:float = 1e-5\n",
    "        self.quantization_range: int = 2 ** (num_bits - 1)\n",
    "    \n",
    "    def binarize_weights(self, weights_gamma: float) -> torch.Tensor:\n",
    "        scaled_weights:torch.Tensor = self.weight / (weights_gamma + self.eps)\n",
    "        binarized_input_no_grad: torch.Tensor = torch.clamp(torch.round(scaled_weights), min=-1, max=1)\n",
    "        binarized_input_with_grad: torch.Tensor = (binarized_input_no_grad - self.weight).detach() + self.weight\n",
    "        return binarized_input_with_grad\n",
    "    \n",
    "    def quantize_activations(self, input:torch.Tensor, input_gamma: float) -> torch.Tensor:\n",
    "        # Equation 4 BitNet paper\n",
    "        quantized_input = torch.clamp(\n",
    "                input * self.quantization_range / input_gamma,\n",
    "                -self.quantization_range + self.eps,\n",
    "                self.quantization_range - self.eps,\n",
    "            )\n",
    "        return quantized_input\n",
    "    \n",
    "    def dequantize_activations(self, input: torch.Tensor, input_gamma: float, beta: float) -> torch.Tensor:\n",
    "        return input * input_gamma * beta / self.quantization_range\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # weights are cast automatically as Int8Params, but the bias has to be cast manually\n",
    "        if self.bias is not None and self.bias.dtype != x.dtype:\n",
    "            self.bias.data = self.bias.data.to(x.dtype)\n",
    "\n",
    "        if getattr(self.weight, 'quant_state', None) is None:\n",
    "            if getattr(self, 'quant_state', None) is not None:\n",
    "                # the quant state got lost when the parameter got converted. This happens for example for fsdp\n",
    "                # since we registered the module, we can recover the state here\n",
    "                assert self.weight.shape[1] == 1\n",
    "                if not isinstance(self.weight, Params4bit):\n",
    "                    self.weight = Params4bit(self.weight, quant_storage=self.quant_storage)\n",
    "                self.weight.quant_state = self.quant_state\n",
    "            else:\n",
    "                print('FP4 quantization state not initialized. Please call .cuda() or .to(device) on the LinearFP4 layer first.')\n",
    "        if not self.compute_type_is_set:\n",
    "            self.set_compute_type(x)\n",
    "            self.compute_type_is_set = True\n",
    "\n",
    "        inp_dtype = x.dtype\n",
    "        if self.compute_dtype is not None:\n",
    "            x = x.to(self.compute_dtype)\n",
    "\n",
    "        input_gamma = x.abs().max().item()\n",
    "        weight_abs_mean = self.weight.float().abs().mean().item()\n",
    "        binarized_weights = self.binarize_weights(weight_abs_mean)\n",
    "        x = self.quantize_activations(x, input_gamma)\n",
    "\n",
    "        bias = None if self.bias is None else self.bias.to(self.compute_dtype)\n",
    "        out = bnb.matmul_4bit(x, binarized_weights.t(), bias=bias, quant_state=self.weight.quant_state)\n",
    "        out = self.dequantize_activations(out, input_gamma, weight_abs_mean)\n",
    "        out = out.to(inp_dtype)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class CiscoHalf(nn.Linear):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        bias: bool = False,\n",
    "        num_bits: int = 8,\n",
    "    ):\n",
    "        super().__init__(in_features, out_features, bias)\n",
    "        self.eps:float = 1e-5\n",
    "        self.quantization_range: int = 2 ** (num_bits - 1) # Q_b in the paper\n",
    "        self.norm: nn.Module = nn.LayerNorm(in_features)\n",
    "\n",
    "\n",
    "    def ste_weights(self, weights_gamma: float) -> torch.Tensor:\n",
    "        eps: float = 1e-8\n",
    "        scaled_weights:torch.Tensor = self.weight / (weights_gamma + eps)\n",
    "        binarized_input_no_grad: torch.Tensor = torch.clamp(torch.round(scaled_weights), min=-1, max=1)\n",
    "        binarized_input_with_grad: torch.Tensor = (binarized_input_no_grad - self.weight).detach() + self.weight\n",
    "        return binarized_input_with_grad.to(torch.float16)\n",
    "\n",
    "\n",
    "    def binarize_weights(self, weights_gamma: float) -> torch.Tensor:\n",
    "        binarized_weights = self.ste_weights(weights_gamma)\n",
    "        return binarized_weights\n",
    "\n",
    "\n",
    "    def quantize_activations(self, _input:torch.Tensor, input_gamma: float) -> torch.Tensor:\n",
    "        # Equation 4 BitNet paper\n",
    "        quantized_input = torch.clamp(\n",
    "                _input * self.quantization_range / input_gamma,\n",
    "                -self.quantization_range + self.eps,\n",
    "                self.quantization_range - self.eps,\n",
    "            )\n",
    "        return quantized_input.to(torch.float16)\n",
    "\n",
    "\n",
    "    def dequantize_activations(self, _input: torch.Tensor, input_gamma: float, beta: float) -> torch.Tensor:\n",
    "        return _input * input_gamma * beta / self.quantization_range\n",
    "\n",
    "\n",
    "    def forward(self, _input: torch.Tensor) -> torch.Tensor:\n",
    "        normalized_input: torch.Tensor = self.norm(_input)\n",
    "        input_gamma: float = normalized_input.abs().max().item()\n",
    "        weight_abs_mean: float = self.weight.abs().mean().item()\n",
    "\n",
    "        binarized_weights = self.binarize_weights(weight_abs_mean)\n",
    "        input_quant = self.quantize_activations(normalized_input, input_gamma)\n",
    "        output = F.linear(input_quant, binarized_weights, self.bias)\n",
    "        output = self.dequantize_activations(output, input_gamma, weight_abs_mean)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class Cisco(nn.Linear):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        bias: bool = False,\n",
    "        num_bits: int = 8,\n",
    "    ):\n",
    "        super().__init__(in_features, out_features, bias)\n",
    "        self.eps:float = 1e-5\n",
    "        self.quantization_range: int = 2 ** (num_bits - 1) # Q_b in the paper\n",
    "        self.norm: nn.Module = nn.LayerNorm(in_features)\n",
    "\n",
    "\n",
    "    def ste_weights(self, weights_gamma: float) -> torch.Tensor:\n",
    "        eps: float = 1e-8\n",
    "        scaled_weights:torch.Tensor = self.weight / (weights_gamma + eps)\n",
    "        binarized_input_no_grad: torch.Tensor = torch.clamp(torch.round(scaled_weights), min=-1, max=1)\n",
    "        binarized_input_with_grad: torch.Tensor = (binarized_input_no_grad - self.weight).detach() + self.weight\n",
    "        return binarized_input_with_grad\n",
    "\n",
    "\n",
    "    def binarize_weights(self, weights_gamma: float) -> torch.Tensor:\n",
    "        binarized_weights = self.ste_weights(weights_gamma)\n",
    "        return binarized_weights\n",
    "\n",
    "\n",
    "    def quantize_activations(self, _input:torch.Tensor, input_gamma: float) -> torch.Tensor:\n",
    "        # Equation 4 BitNet paper\n",
    "        quantized_input = torch.clamp(\n",
    "                _input * self.quantization_range / input_gamma,\n",
    "                -self.quantization_range + self.eps,\n",
    "                self.quantization_range - self.eps,\n",
    "            )\n",
    "        return quantized_input\n",
    "\n",
    "\n",
    "    def dequantize_activations(self, _input: torch.Tensor, input_gamma: float, beta: float) -> torch.Tensor:\n",
    "        return _input * input_gamma * beta / self.quantization_range\n",
    "\n",
    "\n",
    "    def forward(self, _input: torch.Tensor) -> torch.Tensor:\n",
    "        normalized_input: torch.Tensor = self.norm(_input)\n",
    "        input_gamma: float = normalized_input.abs().max().item()\n",
    "        weight_abs_mean: float = self.weight.abs().mean().item()\n",
    "\n",
    "        binarized_weights = self.binarize_weights(weight_abs_mean)\n",
    "        input_quant = self.quantize_activations(normalized_input, input_gamma)\n",
    "        output = F.linear(input_quant, binarized_weights, self.bias)\n",
    "        output = self.dequantize_activations(output, input_gamma, weight_abs_mean)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18f101a1480b40debe1e656b54571582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3f6870474f47778eced4ef2eb3368e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3644d9c207484886a2f2aaafd185e283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b442563a06fd4bbfa8f5cd9361d17f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed comparison matrix:\n",
      "+--------------+----------+-------------+-------------+----------+\n",
      "| Input Size   |   Linear |   BNBLinear |   CiscoHalf |    Cisco |\n",
      "+==============+==========+=============+=============+==========+\n",
      "| (1, 64)      |  3.3e-05 |    0.000491 |    0.000496 | 0.000469 |\n",
      "+--------------+----------+-------------+-------------+----------+\n",
      "| (1, 640)     |  3.3e-05 |    0.000527 |    0.000588 | 0.00053  |\n",
      "+--------------+----------+-------------+-------------+----------+\n",
      "| (1, 6400)    |  3.3e-05 |    0.006594 |    0.009963 | 0.009347 |\n",
      "+--------------+----------+-------------+-------------+----------+\n",
      "| (8, 64)      |  3.1e-05 |    0.000619 |    0.000547 | 0.000424 |\n",
      "+--------------+----------+-------------+-------------+----------+\n",
      "| (8, 640)     |  3.2e-05 |    0.000674 |    0.000647 | 0.000507 |\n",
      "+--------------+----------+-------------+-------------+----------+\n",
      "| (8, 6400)    |  3.8e-05 |    0.007255 |    0.009561 | 0.008994 |\n",
      "+--------------+----------+-------------+-------------+----------+\n",
      "| (128, 64)    |  4e-05   |    0.000549 |    0.000555 | 0.000434 |\n",
      "+--------------+----------+-------------+-------------+----------+\n",
      "| (128, 640)   |  4.5e-05 |    0.000649 |    0.000619 | 0.000582 |\n",
      "+--------------+----------+-------------+-------------+----------+\n",
      "| (128, 6400)  |  3.7e-05 |    0.007325 |    0.009797 | 0.009589 |\n",
      "+--------------+----------+-------------+-------------+----------+\n",
      "| (256, 12800) |  3.6e-05 |    0.024793 |    0.034843 | 0.038679 |\n",
      "+--------------+----------+-------------+-------------+----------+\n",
      "+--------------+----------+-------------+-------------+---------+\n",
      "| Input Size   |   Linear |   BNBLinear |   CiscoHalf |   Cisco |\n",
      "+==============+==========+=============+=============+=========+\n",
      "| Linear       |     1    |        0.01 |        0.01 |    0.01 |\n",
      "+--------------+----------+-------------+-------------+---------+\n",
      "| BNBLinear    |   137.88 |        1    |        0.73 |    0.71 |\n",
      "+--------------+----------+-------------+-------------+---------+\n",
      "| CiscoHalf    |   188.43 |        1.37 |        1    |    0.97 |\n",
      "+--------------+----------+-------------+-------------+---------+\n",
      "| Cisco        |   193.84 |        1.41 |        1.03 |    1    |\n",
      "+--------------+----------+-------------+-------------+---------+\n"
     ]
    }
   ],
   "source": [
    "def test_forward_speed(layer_class, input_sizes, num_runs=1000):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    results = {}\n",
    "\n",
    "    for size in tqdm(input_sizes):\n",
    "        in_features = size[-1]\n",
    "        out_features = size[-1]\n",
    "        layer = layer_class(in_features, out_features).to(device)\n",
    "        input_tensor = torch.randn(size).to(device)\n",
    "\n",
    "        # Warm-up run\n",
    "        _ = layer(input_tensor)\n",
    "\n",
    "        # Timing runs\n",
    "        times = []\n",
    "        for _ in range(num_runs):\n",
    "            start_time = time.perf_counter()\n",
    "            _ = layer(input_tensor)\n",
    "            end_time = time.perf_counter()\n",
    "            times.append(end_time - start_time)\n",
    "\n",
    "        avg_time = sum(times) / num_runs\n",
    "        results[size] = avg_time\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "input_sizes = [\n",
    "    (1, 64),\n",
    "    (1, 640),\n",
    "    (1, 6400),\n",
    "    (8, 64),\n",
    "    (8, 640),\n",
    "    (8, 6400),\n",
    "    (128, 64),\n",
    "    (128, 640),\n",
    "    (128, 6400),\n",
    "    (256, 12800), \n",
    "]\n",
    "\n",
    "layer_classes = [nn.Linear, BNBLinear, CiscoHalf, Cisco]  # Add more layer classes as needed\n",
    "layer_results = {}\n",
    "\n",
    "for layer_class in layer_classes:\n",
    "    layer_results[layer_class.__name__] = test_forward_speed(layer_class, input_sizes)\n",
    "\n",
    "print(\"Speed comparison matrix:\")\n",
    "headers = [\"Input Size\"] + [cls.__name__ for cls in layer_classes]\n",
    "table_data = []\n",
    "\n",
    "for size in input_sizes:\n",
    "    row = [str(size)]\n",
    "    for layer_class in layer_classes:\n",
    "        row.append(f\"{layer_results[layer_class.__name__][size]:.6f}\")\n",
    "    table_data.append(row)\n",
    "\n",
    "print(tabulate(table_data, headers, tablefmt=\"grid\"))\n",
    "\n",
    "table_data = []\n",
    "for layer1 in layer_classes:\n",
    "    row = [layer1.__name__]\n",
    "    for layer2 in layer_classes:\n",
    "        if layer1 == layer2:\n",
    "            row.append(\"1.00\")\n",
    "        else:\n",
    "            time1_sum = sum(layer_results[layer1.__name__].values())\n",
    "            time2_sum = sum(layer_results[layer2.__name__].values())\n",
    "            ratio = time1_sum / time2_sum\n",
    "            row.append(f\"{ratio:.2f}\")\n",
    "    table_data.append(row)\n",
    "\n",
    "print(tabulate(table_data, headers, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
