{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import time\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BitLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    BitLinear module as described in the BitNet architecture.\n",
    "\n",
    "    This module performs a linear transformation with 1-bit quantized weights.\n",
    "    The transformation includes a quantization step, matrix multiplication,\n",
    "    and a subsequent dequantization step. Both the quantization and\n",
    "    dequantization steps utilize learnable parameters gamma and beta.\n",
    "\n",
    "    Attributes:\n",
    "    - in_features: size of each input sample\n",
    "    - out_features: size of each output sample\n",
    "    - gamma: scaling factor for absmax quantization (learnable parameter)\n",
    "    - beta: scaling factor for dequantization (learnable parameter)\n",
    "    - weight: the 1-bit quantized weights of the linear transformation\n",
    "    - bias: the bias term for the linear transformation (optional)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        \"\"\"\n",
    "        Initializes the BitLinear module.\n",
    "\n",
    "        Parameters:\n",
    "        - in_features: An integer, the number of input features.\n",
    "        - out_features: An integer, the number of output features.\n",
    "        - bias: A boolean, whether the layer includes a bias.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "\n",
    "        # Initialize weights and bias\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.randn(out_features))\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "\n",
    "        # Learnable parameters for quantization and dequantization\n",
    "        self.gamma = nn.Parameter(torch.ones(in_features))\n",
    "        self.beta = nn.Parameter(torch.ones(out_features))\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Forward pass of the BitLinear module.\n",
    "\n",
    "        Parameters:\n",
    "        - input: A tensor of shape (batch_size, in_features).\n",
    "\n",
    "        Returns:\n",
    "        - output: A tensor of shape (batch_size, out_features).\n",
    "        \"\"\"\n",
    "        # Apply Layer Normalization\n",
    "        input_norm = F.layer_norm(input, (self.in_features,))\n",
    "\n",
    "        # Absmax Quantization\n",
    "        quant_scale = torch.max(torch.abs(input_norm), dim=1, keepdim=True).values\n",
    "        input_quant = torch.sign(input_norm) * (quant_scale / self.gamma)\n",
    "\n",
    "        # 1-bit Weights Quantization\n",
    "        weight_quant = torch.sign(self.weight)\n",
    "\n",
    "        # MatMul with 1-bit weights using torch.matmul for explicit operation\n",
    "        output = torch.matmul(input_quant, weight_quant.t())\n",
    "\n",
    "        # Adding bias if it exists\n",
    "        if self.bias is not None:\n",
    "            output += self.bias.unsqueeze(0).expand_as(output)\n",
    "\n",
    "        # Dequantization with learnable parameters\n",
    "        output = output * self.beta.unsqueeze(0).expand_as(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5238b11d9bd456b87456237d913c54b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 40\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     28\u001b[0m input_sizes \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     29\u001b[0m     (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m),\n\u001b[0;32m     30\u001b[0m     (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1000\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     (\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m10000\u001b[39m),\n\u001b[0;32m     38\u001b[0m ]\n\u001b[1;32m---> 40\u001b[0m bitlinear_results \u001b[38;5;241m=\u001b[39m \u001b[43mtest_forward_speed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBitLinear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m linear_results \u001b[38;5;241m=\u001b[39m test_forward_speed(nn\u001b[38;5;241m.\u001b[39mLinear, input_sizes)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnn.Linear vs Bitlinear forward function speed comparison:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 12\u001b[0m, in \u001b[0;36mtest_forward_speed\u001b[1;34m(layer_class, input_sizes, num_runs)\u001b[0m\n\u001b[0;32m      9\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(size)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Warm-up run\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Timing runs\u001b[39;00m\n\u001b[0;32m     15\u001b[0m times \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\lhall\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lhall\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 131\u001b[0m, in \u001b[0;36mBitLinear.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    128\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlinear(x_quant, binarized_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# Dequantize activations\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdequantize_activations_groupwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# Return output\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "Cell \u001b[1;32mIn[2], line 106\u001b[0m, in \u001b[0;36mBitLinear.dequantize_activations_groupwise\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdequantize_activations_groupwise\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     97\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m    Dequantizes the activations of the layer in a group-wise manner.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m        Tensor: Dequantized activations tensor.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamma\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mQ_b\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "def test_forward_speed(layer_class, input_sizes, num_runs=100):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    results = {}\n",
    "\n",
    "    for size in tqdm(input_sizes):\n",
    "        in_features = size[-1]\n",
    "        out_features = size[-1]\n",
    "        layer = layer_class(in_features, out_features).to(device)\n",
    "        input_tensor = torch.randn(size).to(device)\n",
    "\n",
    "        # Warm-up run\n",
    "        _ = layer(input_tensor)\n",
    "\n",
    "        # Timing runs\n",
    "        times = []\n",
    "        for _ in range(num_runs):\n",
    "            start_time = time.perf_counter()\n",
    "            _ = layer(input_tensor)\n",
    "            end_time = time.perf_counter()\n",
    "            times.append(end_time - start_time)\n",
    "\n",
    "        avg_time = sum(times) / num_runs\n",
    "        results[size] = avg_time\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "input_sizes = [\n",
    "    (1, 100),\n",
    "    (1, 1000),\n",
    "    (1, 10000),\n",
    "    (10, 100),\n",
    "    (10, 1000),\n",
    "    (10, 10000),\n",
    "    (100, 100),\n",
    "    (100, 1000),\n",
    "    (100, 10000),\n",
    "]\n",
    "\n",
    "bitlinear_results = test_forward_speed(BitLinear, input_sizes)\n",
    "linear_results = test_forward_speed(nn.Linear, input_sizes)\n",
    "\n",
    "print(\"nn.Linear vs Bitlinear forward function speed comparison:\")\n",
    "for size in input_sizes:\n",
    "    linear_time = linear_results[size]\n",
    "    bitlinear_time = bitlinear_results[size]\n",
    "    speed_ratio = bitlinear_time / linear_time\n",
    "    print(f\"Input size: {size}\")\n",
    "    print(f\"  nn.Linear average time: {linear_time:.6f} seconds\")\n",
    "    print(f\"  Bitlinear average time: {bitlinear_time:.6f} seconds\")\n",
    "    print(f\"  nn.Linear is {speed_ratio:.2f} times faster than Bitlinear\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
